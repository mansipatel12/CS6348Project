# -*- coding: utf-8 -*-
"""ML-NLP-Spam-Classification-XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C1ibK-j8t5XO92FQZCzhHlKSAv6QDEFe
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import re
import string
from textblob import TextBlob
import nltk
import spacy
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('wordnet')
from sklearn.preprocessing import LabelEncoder
import re
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import *
from sklearn.model_selection import train_test_split
import csv
# ignore warnings
import warnings
warnings.filterwarnings('ignore')

input_file = "/content/modified_spam.csv"
output_file = "/content/cleaned_spam.csv"

# Step 1: Preprocess the file and fix missing quotes properly
with open(input_file, "r", encoding="utf-8") as infile, open(output_file, "w", encoding="utf-8", newline="") as outfile:
    reader = csv.reader(infile)
    writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)  # Let CSV handle quotes correctly

    # Read the header and clean it
    header = next(reader)  # Read the first row as header
    header = [col.strip() for col in header]  # Remove extra spaces
    if header[:2] != ["Class", "Message"]:  # Ensure correct column names
        header[:2] = ["Class", "Message"]

    writer.writerow(header)  # Write cleaned header

    # Process the rest of the file
    for row in reader:
        if len(row) >= 2:  # Ensure we have at least Class and Message
            writer.writerow([row[0].strip(), row[1].strip()])  # Strip spaces & write

print(f"Fixed CSV saved as: {output_file}")

# Step 2: Load the cleaned CSV into pandas
df = pd.read_csv(output_file, quotechar='"', encoding="utf-8")
print(df.head())

# Check Data Shape
df.shape

df.isnull().sum()

# Find Duplicate
df.duplicated().any()

# drop Duplicates
df.drop_duplicates(inplace=True)

df.duplicated().any()

# Distribution of spam and ham messages
# Calculate the count of each label
category_counts = df['Class'].value_counts()

# Plotting the pie chart
plt.figure(figsize=(8, 8))
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Spam vs. Ham')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Iterate through unique categories for text visualization
for category in df['Class'].unique():
    # Filter the DataFrame for the current category
    filtered_df = df[df['Class'] == category]

    print(filtered_df.columns)
    # Concatenate all text data for the current category
    text = ' '.join(filtered_df['Message'])

    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Plot the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for Category: {category}')
    plt.axis('off')
    plt.show()

# Encode Category column
le = LabelEncoder()
df['Class']=le.fit_transform(df['Class'])
df.head()

print(df.columns)

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Load NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Function to remove HTML tags
def remove_html_tags(text):
    soup = BeautifulSoup(text, 'html.parser')
    return soup.get_text()

# Function to remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Function to remove special characters
def remove_special_characters(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

# Function to remove numeric values
def remove_numeric(text):
    return re.sub(r'\d+', '', text)

# Remove extra white spaces from the 'Text' column
df['Message'] = df['Message'].str.strip()
df.head()

# Define a dictionary of chat word mappings
chat_words = {
    "AFAIK": "As Far As I Know",
    "AFK": "Away From Keyboard",
    "ASAP": "As Soon As Possible",
    "ATK": "At The Keyboard",
    "ATM": "At The Moment",
    "A3": "Anytime, Anywhere, Anyplace",
    "BAK": "Back At Keyboard",
    "BBL": "Be Back Later",
    "BBS": "Be Back Soon",
    "BFN": "Bye For Now",
    "B4N": "Bye For Now",
    "BRB": "Be Right Back",
    "BRT": "Be Right There",
    "BTW": "By The Way",
    "B4": "Before",
    "B4N": "Bye For Now",
    "CU": "See You",
    "CUL8R": "See You Later",
    "CYA": "See You",
    "FAQ": "Frequently Asked Questions",
    "FC": "Fingers Crossed",
    "FWIW": "For What It's Worth",
    "FYI": "For Your Information",
    "GAL": "Get A Life",
    "GG": "Good Game",
    "GN": "Good Night",
    "GMTA": "Great Minds Think Alike",
    "GR8": "Great!",
    "G9": "Genius",
    "IC": "I See",
    "ICQ": "I Seek you (also a chat program)",
    "ILU": "ILU: I Love You",
    "IMHO": "In My Honest/Humble Opinion",
    "IMO": "In My Opinion",
    "IOW": "In Other Words",
    "IRL": "In Real Life",
    "KISS": "Keep It Simple, Stupid",
    "LDR": "Long Distance Relationship",
    "LMAO": "Laugh My A.. Off",
    "LOL": "Laughing Out Loud",
    "LTNS": "Long Time No See",
    "L8R": "Later",
    "MTE": "My Thoughts Exactly",
    "M8": "Mate",
    "NRN": "No Reply Necessary",
    "OIC": "Oh I See",
    "PITA": "Pain In The A..",
    "PRT": "Party",
    "PRW": "Parents Are Watching",
    "QPSA?": "Que Pasa?",
    "ROFL": "Rolling On The Floor Laughing",
    "ROFLOL": "Rolling On The Floor Laughing Out Loud",
    "ROTFLMAO": "Rolling On The Floor Laughing My A.. Off",
    "SK8": "Skate",
    "STATS": "Your sex and age",
    "ASL": "Age, Sex, Location",
    "THX": "Thank You",
    "TTFN": "Ta-Ta For Now!",
    "TTYL": "Talk To You Later",
    "U": "You",
    "U2": "You Too",
    "U4E": "Yours For Ever",
    "WB": "Welcome Back",
    "WTF": "What The F...",
    "WTG": "Way To Go!",
    "WUF": "Where Are You From?",
    "W8": "Wait...",
    "7K": "Sick:-D Laugher",
    "TFW": "That feeling when",
    "MFW": "My face when",
    "MRW": "My reaction when",
    "IFYP": "I feel your pain",
    "TNTL": "Trying not to laugh",
    "JK": "Just kidding",
    "IDC": "I don't care",
    "ILY": "I love you",
    "IMU": "I miss you",
    "ADIH": "Another day in hell",
    "ZZZ": "Sleeping, bored, tired",
    "WYWH": "Wish you were here",
    "TIME": "Tears in my eyes",
    "BAE": "Before anyone else",
    "FIMH": "Forever in my heart",
    "BSAAW": "Big smile and a wink",
    "BWL": "Bursting with laughter",
    "BFF": "Best friends forever",
    "CSL": "Can't stop laughing"
}

def replace_chat_words(text):
    words = text.split()
    words = [chat_words[word.lower()] if word.lower() in chat_words else word for word in words]
    return ' '.join(words)

# Function for spaCy Tokenization, Stopword Removal, and Lemmatization
def preprocess_text(text):
    # Step 1: Convert to lowercase
    text = text.lower().strip()

    # Step 2: Remove HTML tags
    text = remove_html_tags(text)

    # Step 3: Remove punctuation
    text = remove_punctuation(text)

    # Step 4: Remove special characters & numbers
    text = remove_special_characters(text)
    text = remove_numeric(text)

    # Step 5: Replace chat words
    text = replace_chat_words(text)

    # Step 6: Tokenization using spaCy
    doc = nlp(text)

    # Step 7: Remove stop words & apply lemmatization
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

    # Return the cleaned text
    return " ".join(tokens)

# Apply preprocessing to the dataset
df["Message"] = df["Message"].apply(preprocess_text)

# Text Vectorization
#Intlize CountVectorizer
cv = CountVectorizer()

# Fitting CountVectorizer on X
X = cv.fit_transform(df['Message']).toarray()
y = df['Class']

# Train Test Split
X_train, X_test , y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)

# Building Model
# XGBoost Classifier with tuned parameters

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# XGBoost Classifier
xgb_model = XGBClassifier()

# GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters from the grid search
best_params = grid_search.best_params_
print(f"Best hyperparameters: {best_params}")

# Use the best model from the grid search to make predictions
best_xgb_model = grid_search.best_estimator_

# Predict with the best model
xgb_pred = best_xgb_model.predict(X_test)

# Evaluate the model
xgb_accuracy = accuracy_score(y_test, xgb_pred)
xgb_precision = precision_score(y_test, xgb_pred, average='weighted')
xgb_recall = recall_score(y_test, xgb_pred, average='weighted')
xgb_conf_matrix = confusion_matrix(y_test, xgb_pred)

print("XGBoost Classifier with GridSearchCV:")
print(f"The accuracy score of XGBoost Classifier is {xgb_accuracy}")
print(f"The Precision Score is {xgb_precision}")
print(f"The Recall Score is {xgb_recall}")
print(f"The Confusion matrix is \n{xgb_conf_matrix}")

# # Building Model (only run once with best params to save runtime)
# # XGBoost Classifier with tuned parameters
# xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.2, max_depth=5)
# xgb_model.fit(X_train, y_train)
# xgb_pred = xgb_model.predict(X_test)

# xgb_accuracy = accuracy_score(y_test, xgb_pred)
# xgb_precision = precision_score(y_test, xgb_pred, average='weighted')
# xgb_recall = recall_score(y_test, xgb_pred, average='weighted')
# xgb_conf_matrix = confusion_matrix(y_test, xgb_pred)

# print("XGBoost Classifier:")
# print(f"The accuracy score of XGBoost Classifier is {xgb_accuracy}, The Precision Score is {xgb_precision},The Recall Score is {xgb_recall}")
# print(f"The Confusion matrix is \n{xgb_conf_matrix}")
# print("\n")

